\documentclass[acmsmall,review, nonacm]{acmart}

\setcopyright{none}

\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations

\begin{document}

\title{ASE Reflection Paper: From Formal Verification To Test Generation} 

\author{Minsung Cho}
\affiliation{
  \institution{Northeastern University}            %% \institution is required
  \city{Boston}
  \state{Massachusetts}
  \country{USA}                    %% \country is recommended
}
\email{minsung@ccs.neu.edu}          %% \email is recommended

\begin{abstract}
    As software becomes increasingly complex, the assurance that software is correct becomes ever more important. Computer scientists have studied the problem of software correctness through two primary lenses: \textit{formal verification} and \textit{software testing}. This reflection papers details my attempt at understanding the landscape and development of software testing techniques as a researcher in verification, and how the two approaches have influenced one another.
\end{abstract}

\maketitle

\section{Introduction}

As the need for correct software increases, two approaches to writing correct software became prominent research directions.
\begin{enumerate}
  \item \textit{Formal verification} works with mathematical abstractions of programming language behavior to provide a mathematical proof that software behaves correctly. 
  \item \textit{Software testing} aims to validate correct program output under certain inputs to provide empirical confidence that software behaves correctly.
\end{enumerate}

In \cite{parnas1985defense}, David Parnas argues that formal verification is not the correct approach. His argument has two main points: (1) the size of software (in lines of code) that can be verified is small compared to the size that can be robustly tested  and (2) there are some programming languages that lack semantic models that capture all the behavior that a programmer may want to verify.

However, this article is nearly 40 years old! Formal verification has developed a lot since then. Although it's true that many verification tasks are computationally hard, the verification tools we have today can verify entire compilers \cite{kiam2019cakeml} and microprocessors \cite{hardin2010design}, as well as prove powerful theorems in pure mathematics. Although the fact that some languages lack an all-encompassing semantic model is still true today\footnote{whether or not this is a worthwhile research endeavor in programming languages is something I have hot takes on beyond the scope of this paper.}, the fact that testing can detect bugs about, say, concurrency, fairly reliably has not stopped some from falling through the cracks \cite{gates_2023}.

My primary experience as a computer science researcher is in the field of formal methods, of which verification is one of the primary applications. As such, my ill-informed perspective on software testing is biased at best, and skeptical at worst. This reflection paper details my journey into the world of software testing--specfically, software test generation. I first ease into test generation by looking at an application of formal verification, specifically software model checking, to test generation. Then I explore property-based and metamorphic test generation techniques as a weaker application of formal verification. Lastly, I look at \texttt{KLEE}, a symbolic engine for test generation, as a marriage of the two techniques addressed in previous sections.

\textit{Disclaimer.} For the duration of this paper I work with the definition of software test given by \cite{fink1997property}, which is ``a set of executions of a given program using different input data for each execution''. As such, I don't talk much about testing that syntactically changes the program code (e.g. mutation testing).

\section{Formal verification to generate tests}

In 2002, Henzinger, Jhala, Majumdar, and Sutre published a seminal paper \cite{henzinger2002lazy} that describes an \textit{abstract-check-refine} loop to verify properties of \texttt{C} programs. Without getting too far into the details, the loop does the following:
\begin{itemize}
  \item \textit{Abstract}: both the program and the safety properties we want to check are abstractly represented in mathematical language.
  \item \textit{Check}: The abstracted program is checked if it satisfies the abstracted safety property. The way we do this is the bulk of the technical material of \cite{henzinger2002lazy} and beyond the scope of this reflection paper. If we do satisfy the property, then all is well in the world and we terminate the loop.
  \item \textit{Refine}: If the abstracted program does not satisfy the property, we generate an abstract counterexample to our safety property. If this counterexample corresponds to a valid program path, then our program violates the safety property. If not, we need to to add additional predicates in our abstractions to account for this unrealizable counterexample and run the loop again.
\end{itemize}

This loop seems super powerful! Thousands of papers have experimented on extending the features of this algorithm, making it more efficient, and adapting it to more programming languages and abstractions. One such paper is \cite{beyer2004generating}, where the authors use this loop to generate test suites. 

The key insight in \cite{beyer2004generating} is that each abstract counterexample that corresponds to a valid program execution path can be translated into a concrete program test that a software engineer can test against. Furthermore, when the program is checked for the safety property in the \textit{check} step, we can additionally generate more examples that can serve as more tests the program should pass. The hope is that these generated test suites are robust enough that with high confidence it \textbf{fully realizes the safety properties} even if the original code is modified or extended. Furthermore, this method of test suite generation can be useful when we \textbf{do not want to reveal the source code but give strong guarantees of correctness} to a client: we can verify our software, use the verification to generate a test suite to give to a client, who can then test amongst themselves to be assured of software correctness. 

So, what are the pros and cons of this method of test case generation? The main pro is the \textbf{automatic generation} of test cases with \textbf{theoretical guarantees of robustness}. As software model checking quite literally looks at all valid program execution paths, this approach will never miss an edge case. Furthermore, as long as the abstraction is mathematically sound\footnote{in a technical sense}, we can reliably translate abstract examples into real program tests. 

However, there are several cons. The main cons are actually not within the test generation step itself, but within the \textit{abstract-check-refine} loop the test generation relies on. There are flaws with each step of the loop:

\begin{itemize}
  \item The \textit{abstract} step suffers from two flaws. One flaw is that the abstraction used is particular to imperative programs (such as \texttt{C}) and does not translate very well to other programming paradigms. Furthermore, abstractions have the flaw 
  \item The \textit{check} step is a semidecidable procedure: it can \textbf{potentially run forever}. This does not seem like a very desirable property when you want to prove correctness for \textit{any} program. The authors gloss over this point, and do so in subsequent papers, which is a major flaw in this type of loop.
  \item The \textit{refine} step is, fortunately, decidable. However, depending on what logical framework we are working in, SMT queries \textbf{can be very slow}, to the order of doubly-exponential time. Also, if the counterexample fails to be a valid program trace, the heuristics to design what new predicates should be added are empirical and, in particular, are \textbf{not universal}.  
\end{itemize}

\textit{Conclusions.} Formal verification is an extremely strong and desired tool. Knowing that your software is provably correct is the strongest possible guarantee, and generating robust test suites as a consequence of this guarantee is a great perk. However, as this test generation method relies on formal verification, it falls victim to the same problems of formal verification: the lack of a fully abstract model of all possible program behaviors and the computational inefficiency. 

\section{Property-based and metamorphic testing: a middle ground?}

We learned from the previous section that relying on formal verification to generate our tests suffers from some serious issues. Where can we relax our approach to avoid the same issues that full-on formal verification face?

One place we can relax our approach is full automation. Instead of having our test suite be fully automatically generated, \cite{fink1997property} proposes a hybrid method, in which a software engineer serves as an oracle throughout the process. To aid our oracle, \cite{fink1997property} gives our \textit{abstract-check-refine} loop new tools to firmly ground the high level of abstraction into a framework a software engineer can work with.

\begin{itemize}
  \item The \textit{abstract} step now has a new specification language to specify safety properties to lower the level of abstraction. Furthermore, program \textit{slices}, or subprograms, are generated with respect to our property that the oracle can provide a first pass on how important certain program slices are to test.
  \item The \textit{check} step still generates tests based on the program specifications. But, whereas \cite{beyer2004generating} needed to translate abstract counterexamples to actual program variable assignments, this paper skips this step and uses the method of \textit{iterative contexts} to sequentially generate symbolic (input) variable assignments. Of course, the oracle can provide input here as well.
  \item The \textit{refine} step now is a direct comparison between symbolic execution histories of the program and abstract program slices, which identifies gaps in test coverage. Oracle input is necessary here to identify and address these gaps.
\end{itemize}

Note that \cite{fink1997property} never explicitly mentions the \textit{abstract-check-refine} loop. In fact, Henzinger's paper comes five years after Fink's! Fink, instead, outlines an 8-step process of his test generation and evaluation paradigm. One of the outcomes of my reflection that Fink's 8-step process and Henzinger's 3-step loop serve basically the same purpose: provide an end-to-end outline of how to generate tests, evaluate them, and generate new tests based on coverage.

This testing paradigm provides a middle ground between completely automated testing and completely human testing. It seems to be \textbf{the first paper describing an abstraction-guided semi-automated approach to testing}. It benefits from pros borrowing from both approaches. First, since we still work with abstractions of programs, we can potentially detect bugs in edge cases a human may not be able to detect. Second, we have humans evaluating whether or not certain program paths are worth testing or not, which can directly translate to our \textbf{tests checking exactly what software engineers need confidence in}. Third, we \textbf{overcome the potential nonterminating program} by having manual intervention when necessary. 

But, there are still cons! The first main con is that this work \textbf{lacks an implementation} for generic programs, and rather mostly describes a case study on a specific program. The authors admit that implementing their testing paradigm with good heuristics is a ``nontrivial problem'' \cite{fink1997property} and \textbf{leave it to future work to address these heuristic issues}. The other cons look similar to those of \cite{beyer2004generating}. First, the program abstractions they use still suffer from (1) \textbf{paradigm-specific restrictions} and (2) \textbf{semantic inexpressivity}. Next, although we remove semidecidability from the equation, we still have potential inefficiency. Where does this inefficiency stem from? Mostly, it stems from the fact that the \textbf{tests we generate are informed by the structure of the program} that we are testing. At this point, we have failed to separate \textit{concrete tests} that are assignments to program variables giving a certain output by going down a certain program execution path and \textit{symbolic tests} that are constraints on program specifications that make the program behave a certain way\footnote{Can you tell where I'm going with this?}.

\subsection{The original \texttt{QuickCheck}}

Nowadays, when we think of property-based testing, we think of \texttt{QuickCheck}. \texttt{QuickCheck} has been adapted to numerous different programming languages, and sees widescale usage in both research and industry. But what did the original implementation of \texttt{QuickCheck} \cite{claessen2000quickcheck} accomplish?

\begin{enumerate}
  \item It \textbf{automatically tested programs in Haskell, a functional language.} In   
\end{enumerate}

\section{Symbolic testing}

\section{Conclusions}
\bibliography{paper.bib}
\end{document}